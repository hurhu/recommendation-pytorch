{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_criteo_dataset(file_path, test_size=0.3):\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    dense_features = [col for col in data.columns if col[0]=='I']\n",
    "    sparse_features = [col for col in data.columns if col[0]=='C']\n",
    "    \n",
    "    #缺失值填充\n",
    "    data[dense_features] = data[dense_features].fillna(0)\n",
    "    data[sparse_features] = data[sparse_features].fillna('-1')\n",
    "    \n",
    "    #归一化\n",
    "    data[dense_features] = MinMaxScaler().fit_transform(data[dense_features])\n",
    "    #one-hot 编码\n",
    "    data = pd.get_dummies(data)\n",
    "    \n",
    "    #数据集划分\n",
    "    X = data.drop(['label'], axis=1).values\n",
    "    y = data['label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "    return (X_train, y_train), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 13104]) tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = create_criteo_dataset('./train.txt', test_size=0.5)\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(X_train).float(), torch.tensor(y_train.values).float())\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=32)\n",
    "\n",
    "#看下数据\n",
    "for b in iter(train_loader):\n",
    "    print(b[0].shape, b[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMLayer(nn.Module):\n",
    "    def __init__(self, n=10, k=5):\n",
    "        \"\"\"\n",
    "        n:特征维度\n",
    "        k:隐向量维度\n",
    "        \"\"\"\n",
    "        super(FMLayer, self).__init__()\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        self.linear = nn.Linear(self.n, 1) #前两项线性层\n",
    "        self.v = nn.Parameter(torch.randn(self.n, self.k)) #二阶部分的交互矩阵\n",
    "        nn.init.uniform_(self.v, -0.1, 0.1)\n",
    "    def forward(self, x):\n",
    "        #x的维度是(batch_size, n)\n",
    "        linear_part = self.linear(x)\n",
    "        #根据公式计算二阶部分\n",
    "        inter_part1 = torch.mm(x, self.v)\n",
    "        inter_part2 = torch.mm(torch.pow(x, 2), torch.pow(self.v, 2))\n",
    "        inter = 0.5 * torch.sum(torch.sub(torch.pow(inter_part1, 2), inter_part2), 1, keepdim=True)\n",
    "        output = linear_part + inter\n",
    "        output = torch.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_layer(nn.Module):\n",
    "    def __init__(self, hidden_units, input_dim, dropout=0.):\n",
    "        super(DNN_layer, self).__init__()\n",
    "        self.dnn_network = nn.ModuleList([nn.Linear(layer[0], layer[1]) for layer in list(zip(hidden_units[:-1], hidden_units[1:]))])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.final_linear = nn.Linear(hidden_units[-1], 1)\n",
    "    def forward(self, x):\n",
    "        for linear in self.dnn_network:\n",
    "            x = linear(x)\n",
    "            x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        output = self.final_linear(x)\n",
    "        output = torch.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FMLayer(\n",
       "  (linear): Linear(in_features=13104, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#构建FM模型\n",
    "n = X_train.shape[1]\n",
    "k = 8\n",
    "fm_model = FMLayer(n, k)\n",
    "fm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5297],\n",
      "        [0.4689],\n",
      "        [0.4627],\n",
      "        [0.4739],\n",
      "        [0.4466],\n",
      "        [0.5589],\n",
      "        [0.4697],\n",
      "        [0.5083],\n",
      "        [0.5216],\n",
      "        [0.4904],\n",
      "        [0.4805],\n",
      "        [0.4588],\n",
      "        [0.5141],\n",
      "        [0.4549],\n",
      "        [0.4475],\n",
      "        [0.5383],\n",
      "        [0.6233],\n",
      "        [0.5263],\n",
      "        [0.5061],\n",
      "        [0.5257],\n",
      "        [0.4598],\n",
      "        [0.4690],\n",
      "        [0.4353],\n",
      "        [0.5182],\n",
      "        [0.4684],\n",
      "        [0.4338],\n",
      "        [0.4765],\n",
      "        [0.4684],\n",
      "        [0.4989],\n",
      "        [0.5701],\n",
      "        [0.4763],\n",
      "        [0.4953]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "#测试一下FM模型\n",
    "for fea, label in iter(train_loader):\n",
    "    fm_out = fm_model(fea)\n",
    "    print(fm_out)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练和评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_training.........\n",
      "[step=10] loss: 0.670, auc: 0.508\n",
      "[step=20] loss: 0.644, auc: 0.552\n",
      "[step=30] loss: 0.625, auc: 0.569\n",
      "[step=10] loss: 0.533, auc: 0.852\n",
      "[step=20] loss: 0.514, auc: 0.865\n",
      "[step=30] loss: 0.500, auc: 0.879\n",
      "[step=10] loss: 0.439, auc: 0.972\n",
      "[step=20] loss: 0.434, auc: 0.964\n",
      "[step=30] loss: 0.434, auc: 0.948\n",
      "[step=10] loss: 0.397, auc: 0.975\n",
      "[step=20] loss: 0.386, auc: 0.973\n",
      "[step=30] loss: 0.374, auc: 0.976\n",
      "[step=10] loss: 0.326, auc: 0.990\n",
      "[step=20] loss: 0.325, auc: 0.992\n",
      "[step=30] loss: 0.317, auc: 0.989\n",
      "[step=10] loss: 0.278, auc: 0.995\n",
      "[step=20] loss: 0.265, auc: 0.995\n",
      "[step=30] loss: 0.261, auc: 0.996\n",
      "[step=10] loss: 0.207, auc: 1.000\n",
      "[step=20] loss: 0.213, auc: 0.999\n",
      "[step=30] loss: 0.210, auc: 0.998\n",
      "[step=10] loss: 0.176, auc: 0.999\n",
      "[step=20] loss: 0.169, auc: 0.999\n",
      "[step=30] loss: 0.167, auc: 1.000\n",
      "[step=10] loss: 0.141, auc: 1.000\n",
      "[step=20] loss: 0.140, auc: 1.000\n",
      "[step=30] loss: 0.134, auc: 1.000\n",
      "[step=10] loss: 0.114, auc: 1.000\n",
      "[step=20] loss: 0.109, auc: 1.000\n",
      "[step=30] loss: 0.107, auc: 1.000\n",
      "fm model Finished Training\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "log_step_freq = 10\n",
    "def auc(y_pred, y_true):\n",
    "    pred = y_pred.data\n",
    "    y = y_true.data\n",
    "    return roc_auc_score(y, pred)\n",
    "\n",
    "loss_func = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(params=fm_model.parameters(), lr=0.001)\n",
    "metric_func = auc\n",
    "metric_name = 'auc'\n",
    "\n",
    "print('fm start_training.........')\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    \n",
    "    # 训练阶段\n",
    "    fm_model.train()\n",
    "    fm_loss_sum = 0.0\n",
    "    fm_metric_sum = 0.0\n",
    "    step = 1\n",
    "    \n",
    "    for step, (features, labels) in enumerate(train_loader, 1):\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 正向传播\n",
    "        predictions = fm_model(features);\n",
    "        loss = loss_func(predictions, labels)\n",
    "        try:\n",
    "            metric = metric_func(predictions, labels)\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 打印batch级别日志\n",
    "        fm_loss_sum += loss.item()\n",
    "        fm_metric_sum += metric.item()\n",
    "        if step % log_step_freq == 0:\n",
    "            print((\"[step=%d] loss: %.3f, \" + metric_name + \": %.3f\") % (step, fm_loss_sum/step, fm_metric_sum/step));\n",
    "\n",
    "print('fm model Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 104832]) tensor([0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "#先用fm中的交互矩阵v初始化dnn的embedding部分,换种思路，就是用fm中生成的权重先对原始数据做转换(因为embedding其实是一个mlp)\n",
    "#生成用于dnn的数据\n",
    "new_X_train = torch.unsqueeze(torch.tensor(X_train).float(), dim=2)\n",
    "v = fm_model.v\n",
    "new_X_train1 = torch.mul(new_X_train, v).reshape(-1, v.shape[0]*v.shape[1])\n",
    "#在这里一定要用numpy作为中间存储，不然会和fm的图连在一起，后面的dnn的网络图没法训练\n",
    "new_X_trains = new_X_train1.data.numpy()\n",
    "new_train_dataset = TensorDataset(torch.tensor(new_X_trains).float(), torch.tensor(y_train.values).float())\n",
    "new_train_loader = DataLoader(new_train_dataset, shuffle=True, batch_size=32)\n",
    "\n",
    "#看下数据\n",
    "for b in iter(new_train_loader):\n",
    "    print(b[0].shape, b[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNN_layer(\n",
       "  (dnn_network): ModuleList(\n",
       "    (0): Linear(in_features=104832, out_features=128, bias=True)\n",
       "    (1): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (3): Linear(in_features=32, out_features=16, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.0)\n",
       "  (final_linear): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#构建dnn模型\n",
    "input_dim = new_X_trains.shape[1]\n",
    "print(input_dim)\n",
    "hidden_units = [input_dim, 128, 64, 32, 16]\n",
    "dnn_model = DNN_layer(hidden_units, input_dim)\n",
    "dnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4432],\n",
      "        [0.4432],\n",
      "        [0.4432],\n",
      "        [0.4432],\n",
      "        [0.4432],\n",
      "        [0.4432],\n",
      "        [0.4432],\n",
      "        [0.4432],\n",
      "        [0.4432],\n",
      "        [0.4432],\n",
      "        [0.4432],\n",
      "        [0.4432],\n",
      "        [0.4432],\n",
      "        [0.4432],\n",
      "        [0.4432],\n",
      "        [0.4432],\n",
      "        [0.4432],\n",
      "        [0.4432],\n",
      "        [0.4432],\n",
      "        [0.4432],\n",
      "        [0.4432],\n",
      "        [0.4432],\n",
      "        [0.4432],\n",
      "        [0.4432],\n",
      "        [0.4432],\n",
      "        [0.4432],\n",
      "        [0.4432],\n",
      "        [0.4432],\n",
      "        [0.4432],\n",
      "        [0.4432],\n",
      "        [0.4432],\n",
      "        [0.4432]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "#测试一下dnn模型\n",
    "for fea, label in iter(new_train_loader):\n",
    "    dnn_out = dnn_model(fea)\n",
    "    print(dnn_out)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_training.........\n",
      "[step=10] loss: 0.643, auc: 0.592\n",
      "[step=20] loss: 0.639, auc: 0.595\n",
      "[step=30] loss: 0.633, auc: 0.597\n",
      "[step=10] loss: 0.631, auc: 0.720\n",
      "[step=20] loss: 0.630, auc: 0.753\n",
      "[step=30] loss: 0.629, auc: 0.742\n",
      "[step=10] loss: 0.628, auc: 0.794\n",
      "[step=20] loss: 0.622, auc: 0.778\n",
      "[step=30] loss: 0.620, auc: 0.779\n",
      "[step=10] loss: 0.615, auc: 0.789\n",
      "[step=20] loss: 0.608, auc: 0.823\n",
      "[step=30] loss: 0.602, auc: 0.809\n",
      "[step=10] loss: 0.594, auc: 0.851\n",
      "[step=20] loss: 0.580, auc: 0.836\n",
      "[step=30] loss: 0.579, auc: 0.828\n",
      "[step=10] loss: 0.534, auc: 0.832\n",
      "[step=20] loss: 0.543, auc: 0.852\n",
      "[step=30] loss: 0.548, auc: 0.856\n",
      "[step=10] loss: 0.528, auc: 0.897\n",
      "[step=20] loss: 0.507, auc: 0.880\n",
      "[step=30] loss: 0.502, auc: 0.874\n",
      "[step=10] loss: 0.490, auc: 0.893\n",
      "[step=20] loss: 0.483, auc: 0.889\n",
      "[step=30] loss: 0.480, auc: 0.894\n",
      "[step=10] loss: 0.455, auc: 0.947\n",
      "[step=20] loss: 0.456, auc: 0.910\n",
      "[step=30] loss: 0.447, auc: 0.928\n",
      "[step=10] loss: 0.403, auc: 0.952\n",
      "[step=20] loss: 0.415, auc: 0.954\n",
      "[step=30] loss: 0.412, auc: 0.952\n",
      "dnn model Finished Training\n"
     ]
    }
   ],
   "source": [
    "# 训练dnn模型\n",
    "epochs = 10\n",
    "log_step_freq = 10\n",
    "\n",
    "loss_func1 = nn.BCELoss()\n",
    "optimizer1 = torch.optim.Adam(params=dnn_model.parameters(), lr=0.0001)\n",
    "metric_func = auc\n",
    "metric_name = 'auc'\n",
    "\n",
    "print('start_training.........')\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    \n",
    "    # 训练阶段\n",
    "    dnn_model.train()\n",
    "    dnn_loss_sum = 0.0\n",
    "    dnn_metric_sum = 0.0\n",
    "    step = 1\n",
    "    \n",
    "    for step, (features, labels) in enumerate(new_train_loader, 1):\n",
    "        # 梯度清零\n",
    "        optimizer1.zero_grad()\n",
    "        \n",
    "        # 正向传播\n",
    "        predictions = dnn_model(features);\n",
    "        loss = loss_func1(predictions, labels)\n",
    "        try:\n",
    "            metric = metric_func(predictions, labels)\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "        \n",
    "        # 打印batch级别日志\n",
    "        dnn_loss_sum += loss.item()\n",
    "        dnn_metric_sum += metric.item()\n",
    "        if step % log_step_freq == 0:\n",
    "            print((\"[step=%d] loss: %.3f, \" + metric_name + \": %.3f\") % (step, dnn_loss_sum/step, dnn_metric_sum/step));\n",
    "\n",
    "print('dnn model Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用测试集评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc_fm:0.630 test_acc_fm:0.795\n",
      "test_auc_fnn:0.643, test_acc_fnn:0.802\n"
     ]
    }
   ],
   "source": [
    "test_y = y_test.values\n",
    "y_pred_fm_probs = fm_model(torch.tensor(X_test).float())\n",
    "y_pred_fm = torch.where(y_pred_fm_probs>0.5, torch.ones_like(y_pred_fm_probs), torch.zeros_like(y_pred_fm_probs))\n",
    "\n",
    "test_auc_fm = roc_auc_score(test_y, y_pred_fm_probs.data.numpy())\n",
    "test_acc_fm = accuracy_score(test_y, y_pred_fm.data.numpy())\n",
    "print('test_auc_fm:%.3f test_acc_fm:%.3f'%(test_auc_fm, test_acc_fm))\n",
    "\n",
    "\n",
    "\n",
    "new_X_test = torch.unsqueeze(torch.tensor(X_test).float(), dim=2)\n",
    "v = fm_model.v\n",
    "new_X_test1 = torch.mul(new_X_test, v).reshape(-1, v.shape[0]*v.shape[1])\n",
    "y_pred_fnn_probs = dnn_model(new_X_test1.float())\n",
    "y_pred_fnn = torch.where(y_pred_fnn_probs>0.5, torch.ones_like(y_pred_fnn_probs), torch.zeros_like(y_pred_fnn_probs))\n",
    "\n",
    "test_auc_fnn = roc_auc_score(test_y, y_pred_fnn_probs.data.numpy())\n",
    "test_acc_fnn = accuracy_score(test_y, y_pred_fnn.data.numpy())\n",
    "print('test_auc_fnn:%.3f, test_acc_fnn:%.3f'%(test_auc_fnn, test_acc_fnn))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('anaconda3': virtualenv)",
   "language": "python",
   "name": "python3610jvsc74a57bd06b1511be9513fc759ecfb232ab57c7974d575220b70bfe4e49edc0a0cfa638a0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
