{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoryEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, min_cnt=5, word2idx=None, idx2word=None):\n",
    "        super().__init__()\n",
    "        self.min_cnt = min_cnt\n",
    "        self.word2idx = word2idx if word2idx else dict()\n",
    "        self.idx2word = idx2word if idx2word else dict()\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        if not self.word2idx:\n",
    "            counter = Counter(np.asarray(x).ravel())\n",
    "            \n",
    "            selected_terms = sorted(list(filter(lambda x:counter[x] >= self.min_cnt, counter)))\n",
    "            \n",
    "            self.word2idx = dict(zip(selected_terms, range(1, len(selected_terms) + 1)))\n",
    "            self.word2idx['__PAD__'] = 0\n",
    "            if '__UNKNOWN__' not in self.word2idx:\n",
    "                self.word2idx['__UNKNOWN__'] = len(self.word2idx)\n",
    "        if not self.idx2word:\n",
    "            self.idx2word = {index:word for word, index in self.word2idx.items()}\n",
    "        return self\n",
    "    def transform(self, x):\n",
    "        transformed_x = list()\n",
    "        for term in np.asarray(x).ravel():\n",
    "            try:\n",
    "                transformed_x.append(self.word2idx[term])\n",
    "            except KeyError:\n",
    "                transformed_x.append(self.word2idx['__UNKNOWN__'])\n",
    "        return np.asarray(transformed_x, dtype=np.float64)\n",
    "    def dimension(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "class SequenceEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, sep=' ', min_cnt=5, max_len=None, word2idx=None, idx2word=None):\n",
    "        super().__init__()\n",
    "        self.sep = sep\n",
    "        self.min_cnt = min_cnt\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        self.word2idx = word2idx if word2idx else dict()\n",
    "        self.idx2word = idx2word if idx2word else dict()\n",
    "    def fit(self, x, y=None):\n",
    "        if not self.word2idx:\n",
    "            counter = Counter()\n",
    "            max_len = 0\n",
    "            for sequence in np.array(x).ravel():\n",
    "                words = sequence.split(self.sep)\n",
    "                counter.update(words)\n",
    "                max_len = max(max_len, len(words))\n",
    "            if self.max_len is None:\n",
    "                self.max_len = max_len\n",
    "            \n",
    "            #drop rate words\n",
    "            words = sorted(list(filter(lambda x:counter[x] >= self.min_cnt, counter)))\n",
    "            \n",
    "            self.word2idx = dict(zip(words, range(1, len(words)+1)))\n",
    "            self.word2idx['__PAD__'] = 0\n",
    "            if '__UNKNOWN__' not in self.word2idx:\n",
    "                self.word2idx['__UNKNOWN__'] = len(self.word2idx)\n",
    "        if not self.idx2word:\n",
    "            self.idx2word = {index:word for word, index in self.word2idx.items()}\n",
    "        \n",
    "        if not self.max_len:\n",
    "            max_len = 0\n",
    "            for sequence in np.array(x).ravel():\n",
    "                words = sequence.split(self.sep)\n",
    "                max_len = max(max_len, len(words))\n",
    "            self.max_len = max_len\n",
    "        return self\n",
    "    def transform(self, x):\n",
    "        transformed_x = list()\n",
    "        for sequence in np.asarray(x).ravel():\n",
    "            words = list()\n",
    "            for word in sequence.split(self.sep):\n",
    "                try:\n",
    "                    words.append(self.word2idx[word])\n",
    "                except KeyError:\n",
    "                    words.append(self.word2idx['__UNKNOWN__'])\n",
    "            transformed_x.append(np.asarray(words[0:self.max_len], dtype=np.int64))\n",
    "        return np.asarray(transformed_x, dtype=object)\n",
    "    def dimension(self):\n",
    "        return len(self.word2idx)\n",
    "    def max_length(self):\n",
    "        return self.max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 150.93it/s]\n",
      " 67%|██████▋   | 2/3 [00:00<00:00, 18.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess number features...\n",
      "preprocess category features...\n",
      "preprocess sequence features....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 17.02it/s]\n"
     ]
    }
   ],
   "source": [
    "dftrain = pd.read_csv('./train.csv')\n",
    "dfval = pd.read_csv('./test.csv')\n",
    "\n",
    "for col in ['movieId', 'histHighRatedMovieIds', 'negHistMovieIds', 'genres']:\n",
    "    dftrain[col] = dftrain[col].astype(str)\n",
    "    dfval[col] = dfval[col].astype(str)\n",
    "\n",
    "num_features = ['age']\n",
    "cat_features = ['gender', 'movieId', 'occupation', 'zipCode']\n",
    "seq_features = ['genres', 'histHighRatedMovieIds', 'negHistMovieIds']\n",
    "\n",
    "num_pipe = Pipeline(steps=[('impute',SimpleImputer()), ('quantile', QuantileTransformer())])\n",
    "\n",
    "encoders = {}\n",
    "print('preprocess number features...')\n",
    "dftrain[num_features] = num_pipe.fit_transform(dftrain[num_features]).astype(np.float32)\n",
    "dfval[num_features] = num_pipe.fit_transform(dfval[num_features]).astype(np.float32)\n",
    "\n",
    "print('preprocess category features...')\n",
    "for col in tqdm(cat_features):\n",
    "    encoders[col] = CategoryEncoder(min_cnt=5)\n",
    "    dftrain[col] = encoders[col].fit_transform(dftrain[col])\n",
    "    dfval[col] = encoders[col].transform(dfval[col])\n",
    "print('preprocess sequence features....')\n",
    "for col in tqdm(seq_features):\n",
    "    encoders[col] = SequenceEncoder(sep='|',min_cnt=5)\n",
    "    dftrain[col] = encoders[col].fit_transform(dftrain[col])\n",
    "    dfval[col] = encoders[col].transform(dfval[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8964, 0.8078])\n",
      "tensor([2, 2])\n",
      "tensor([[5, 0, 0, 0, 0, 0],\n",
      "        [6, 0, 0, 0, 0, 0]], dtype=torch.int32)\n",
      "torch.Size([128])\n",
      "<class 'dict'>\n",
      "dict_keys(['age', 'gender', 'movieId', 'occupation', 'zipCode', 'genres', 'histHighRatedMovieIds', 'negHistMovieIds', 'label'])\n",
      "{'gender': 4, 'movieId': 280, 'occupation': 23, 'zipCode': 124, 'genres': 20, 'histHighRatedMovieIds': 1791, 'negHistMovieIds': 3868}\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from itertools import chain\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Df2Dataset(Dataset):\n",
    "    def __init__(self, dfdata, num_features, cat_features, seq_features, encoders, label_col='label'):\n",
    "        self.dfdata = dfdata\n",
    "        self.num_features = num_features\n",
    "        self.cat_features = cat_features\n",
    "        self.seq_features = seq_features\n",
    "        self.encoders = encoders\n",
    "        self.label_col = label_col\n",
    "        self.size = len(self.dfdata)\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    @staticmethod\n",
    "    def pad_sequence(sequence, max_length):\n",
    "        padded_seq = np.zeros(max_length, np.int32)\n",
    "        padded_seq[0:sequence.shape[0]] = sequence\n",
    "        return padded_seq\n",
    "    def __getitem__(self, idx):\n",
    "        record = OrderedDict()\n",
    "        for col in self.num_features:\n",
    "            record[col] = self.dfdata[col].iloc[idx].astype(np.float32)\n",
    "        \n",
    "        for col in self.cat_features:\n",
    "            record[col] = self.dfdata[col].iloc[idx].astype(np.int64)\n",
    "        \n",
    "        for col in self.seq_features:\n",
    "            seq = self.dfdata[col].iloc[idx]\n",
    "            max_length = self.encoders[col].max_length()\n",
    "            record[col] = Df2Dataset.pad_sequence(seq, max_length)\n",
    "        if self.label_col is not None:\n",
    "            record['label'] = self.dfdata[self.label_col].iloc[idx].astype(np.float32)\n",
    "        return record\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return np.ceil(self.size / batch_size)\n",
    "\n",
    "ds_train = Df2Dataset(dftrain, num_features, cat_features, seq_features, encoders)\n",
    "ds_val = Df2Dataset(dfval, num_features, cat_features, seq_features, encoders)\n",
    "dl_train = DataLoader(ds_train, batch_size=128, shuffle=True)\n",
    "df_val = DataLoader(ds_val,  batch_size=128, shuffle=False)\n",
    "\n",
    "cat_nums = {k:v.dimension() for k,v in encoders.items()}\n",
    "\n",
    "for batch in dl_train:\n",
    "    print(batch['age'][0:2])\n",
    "    print(batch['gender'][0:2])\n",
    "    print(batch['genres'][0:2])\n",
    "    print(batch['label'].shape)\n",
    "    print(type(batch))\n",
    "    print(batch.keys())\n",
    "    break\n",
    "print(cat_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from collections import OrderedDict\n",
    "\n",
    "class MaxPooling(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(MaxPooling, self).__init__()\n",
    "        self.dim = dim\n",
    "    def forward(self, x):\n",
    "        return torch.max(x, self.dim)[0]\n",
    "class SumPooling(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(SumPooling, slef).__init__()\n",
    "        self.dim = dim\n",
    "    def forward(self, x):\n",
    "        return torch.sum(x, self.dim)\n",
    "#同DIN，Dice激活函数\n",
    "class Dice(nn.Module):\n",
    "    def __init__(self, emb_size, dim=2, epsilon=1e-8):\n",
    "        super(Dice, self).__init__()\n",
    "        assert dim == 2 or dim == 3\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(emb_size, eps=epsilon)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dim = dim\n",
    "#          self.alpha = nn.Parameter(torch.zeros((emb_size,))) if self.dim == 2 else nn.Parameter(torch.zeros((emb_size, 1)))\n",
    "\n",
    "        self.alpha = nn.Parameter(torch.zeros(emb_size)) if self.dim == 2 else nn.Parameter(torch.zeros(emb_size,1))\n",
    "    def forward(self, x):\n",
    "        assert x.dim() == self.dim\n",
    "        if self.dim == 2:\n",
    "            x_p = self.sigmoid(self.bn(x))\n",
    "            out = self.alpha * (1 - x_p) * x + x_p * x\n",
    "        else:\n",
    "            x = torch.transpose(x, 1, 2)\n",
    "            x_p = self.sigmoid(self.bn(x))\n",
    "            out = self.alpha * (1 - x_p) * x + x_p * x\n",
    "            out = torch.transpose(out, 1, 2)\n",
    "        return out\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "def get_activation_layer(name, hidden_size=None, dice_dim=2):\n",
    "    name = name.lower()\n",
    "    name_dict = {x.lower():x for x in dir(nn) if '__' not in x and 'Z'>=x[0]>='A'}\n",
    "    if name=='linear':\n",
    "        return Identity()\n",
    "    elif name == 'dice':\n",
    "        assert dice_dim\n",
    "        return Dice(hidden_size, dice_dim)\n",
    "    else:\n",
    "        assert name in name_dict, f'activation type {name} not supported!'\n",
    "        return getattr(nn, name_dict[name])()\n",
    "def init_weights(model):\n",
    "    if isinstance(model, nn.Linear):\n",
    "        if model.weight is not None:\n",
    "            nn.init.kaiming_uniform_(model.weight.data)\n",
    "        if model.bias is not None:\n",
    "            nn.init.normal_(model.bias.data)\n",
    "    elif isinstance(model, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):\n",
    "        if model.weight is not None:\n",
    "            nn.init.normal_(model.weight.data, mean=1, std=0.02)\n",
    "        if model.bias is not None:\n",
    "            nn.init.constant_(model.bias.data, 0)\n",
    "    else:\n",
    "        pass\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, dropout=0.0, batchnorm=True, activation='relu'):\n",
    "        super(MLP, self).__init__()\n",
    "        modules = OrderedDict()\n",
    "        previous_size = input_size\n",
    "        for index, hidden_layer in enumerate(hidden_layers):\n",
    "            modules[f\"dense{index}\"] = nn.Linear(previous_size, hidden_layer)\n",
    "            if batchnorm:\n",
    "                modules[f\"batchnorm{index}\"] = nn.BatchNorm1d(hidden_layer)\n",
    "            if activation:\n",
    "                modules[f\"activation{index}\"] = get_activation_layer(activation, hidden_layer, 2)\n",
    "            if dropout:\n",
    "                modules[f\"dropout{index}\"] = nn.Dropout(dropout)\n",
    "            previous_size = hidden_layer\n",
    "        self.mlp = nn.Sequential(modules)\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "class AttentionGRUCell(nn.Module):\n",
    "    #AGRU的实现\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        #Wn就是公式中的Wh\n",
    "        #(Wr|Wn)\n",
    "        self.weight_ih = nn.Parameter(torch.Tensor(2 * hidden_size, input_size))\n",
    "        #Un就是公式中的Uh\n",
    "        #(Ur|Un)\n",
    "        self.weight_hh = nn.Parameter(torch.Tensor(2 * hidden_size, input_size))\n",
    "        if bias:\n",
    "            #(b_ir|b_in)\n",
    "            self.bias_ih = nn.Parameter(torch.Tensor(2 * hidden_size))\n",
    "            #(b_hr|b_hn)\n",
    "            self.bias_hh = nn.Parameter(torch.Tensor(2 * hidden_size))\n",
    "        else:\n",
    "            self.register_parameter('bias_ih', None)\n",
    "            self.register_parameter('bias_hh', None)\n",
    "        self.reset_parameters()\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / (self.hidden_size) ** 0.5\n",
    "        for weight in self.parameters():\n",
    "            nn.init.uniform_(weight, -stdv, stdv)\n",
    "    def forward(self, x, hx, att_score):\n",
    "        gi = F.linear(x, self.weight_ih, self.bias_ih)\n",
    "        gh = F.linear(hx, self.weight_hh, self.bias_hh)\n",
    "        i_r, i_n = gi.chunk(2,1) #i_r，rt公式中it那项的结果加上偏置项结果，i_n是ht波浪线第一项结果\n",
    "        h_r, h_n = gh.chunk(2,1)#h_r,rt那项h(t-1)那项的结果加上偏置项结果, h_n是ht波浪线第二项rt后面的结果\n",
    "        \n",
    "        resetgate = torch.sigmoid(i_r + h_r) #重置门，\n",
    "        newgate = torch.tanh(i_n + resetgate * h_n)  #ht波浪线结果\n",
    "        att_score = att_score.view(-1,1)\n",
    "        hy = (1. - att_score) * hx + att_score * newgate #attention score替换了更新门，直接作用于隐藏状态\n",
    "        return hy\n",
    "class AttentionUpdateGateGRUCell(nn.Module):\n",
    "    #AUGRU的实现\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        #(Wu|Wr|Wn) Wn就是公式中的Wh\n",
    "        self.weight_ih = nn.Parameter(torch.Tensor(3 * hidden_size, input_size))\n",
    "        #(Uu|Ur|Un)  Un就是公式中的Uh\n",
    "        self.weight_hh = nn.Parameter(torch.Tensor(3 * hidden_size, hidden_size))\n",
    "        if bias:\n",
    "            #(b_iu|b_ir|b_in)\n",
    "            self.bias_ih = nn.Parameter(torch.Tensor(3 * hidden_size))\n",
    "            #(b_hu|b_hr|b_hn)\n",
    "            self.bias_hh = nn.Parameter(torch.Tensor(3 * hidden_size))\n",
    "        else:\n",
    "            self.register_parameter('bias_ih', None)\n",
    "            self.register_parameter('bias_hh', None)\n",
    "        self.reset_parameters()\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / (self.hidden_size) ** 0.5\n",
    "        for weight in self.parameters():\n",
    "            nn.init.uniform_(weight, -stdv, stdv)\n",
    "    def forward(self, x, hx, att_score):\n",
    "        gi = F.linear(x, self.weight_ih, self.bias_ih)\n",
    "        gh = F.linear(hx, self.weight_hh, self.bias_hh)\n",
    "        i_u, i_r, i_n = gi.chunk(3,1) #i_u更新门中的第一项，i_r重置门中的第一项,i_n是当前状态信息计算中的第一项\n",
    "        h_u, h_r, h_n = gi.chunk(3,1)#h_u更新们中第二项，h_r重置门中的第二项，h_n是当前状态信息计算中的第二项rt后的项\n",
    "        \n",
    "        updategate = torch.sigmoid(i_u + h_u)  #更新门\n",
    "        resetgate = torch.sigmoid(i_r + h_r)  #重置门\n",
    "        newgate = torch.tanh(i_n + resetgate * h_n)#当前状态信息计算\n",
    "        \n",
    "        updategate = att_score.view(-1,1) * updategate  #attention score作用于更新门\n",
    "        hy = (1 - updategate)*hx + updategate*newgate\n",
    "        return hy\n",
    "#兴趣演化层\n",
    "class DynamicGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bias=True, gru_type='AGRU'):\n",
    "        super(DynamicGRU, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if gru_type == 'AGRU':\n",
    "            self.rnn = AttentionGRUCell(input_size, hidden_size, bias)\n",
    "        elif gru_type == 'AUGRU':\n",
    "            self.rnn = AttentionUpdateGateGRUCell(input_size, hidden_size, bias)\n",
    "    def forward(self, x, att_scores, hx=None):\n",
    "        is_packed_input = isinstance(x, nn.utils.rnn.PackedSequence)\n",
    "        if not is_packed_input:\n",
    "            raise NotImplementedError('DynamicGRU only supports packed input')\n",
    "        is_packed_att_scores = isinstance(att_scores, nn.utils.rnn.PackedSequence)\n",
    "        if not is_packed_att_scores:\n",
    "            raise NotImplementedError('DynamicGRU only packed att_scores')\n",
    "        x, batch_sizes, sorted_indices, unsorted_indices = x\n",
    "        att_scores, _,_,_= att_scores\n",
    "        \n",
    "        max_batch_size = batch_sizes[0]\n",
    "        max_batch_size = int(max_batch_size)\n",
    "        \n",
    "        if hx is None:\n",
    "            hx = torch.zeros(max_batch_size, self.hidden_size, dtype=x.dtype, device=x.device)\n",
    "        outputs = torch.zeros(x.size(0), self.hidden_size, dtype=x.dtype, device=x.device)\n",
    "        \n",
    "        begin = 0\n",
    "        for batch in batch_sizes:\n",
    "            new_hx = self.rnn(x[begin:begin+batch], hx[0:batch], att_scores[begin:begin+batch])\n",
    "            outputs[begin:begin+batch] = new_hx\n",
    "            hx = new_hx\n",
    "            begin += batch\n",
    "        return nn.utils.rnn.PackedSequence(outputs, batch_sizes, sorted_indices, unsorted_indices)\n",
    "#注意力层，用一个mlp网络来得到了每个兴趣抽取输出与目标的关系\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, dropout=0.0, batchnorm=True, activation='prelu', return_scores=False):\n",
    "        super().__init__()\n",
    "        self.return_scores = return_scores\n",
    "        \n",
    "        self.mlp = MLP(input_size=input_size * 4, hidden_layers=hidden_layers, dropout=dropout, batchnorm=batchnorm, activation=activation)\n",
    "#         print(\"attention self.mlp\")\n",
    "#         print(self.mlp)\n",
    "        self.fc = nn.Linear(hidden_layers[-1], 1)\n",
    "    def forward(self, query, keys, keys_length):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ------------\n",
    "        query:2D tensor,[Batch,Hidden]\n",
    "        keys:3D tensor, [Batch, Time, Hidden]\n",
    "        keys_length:1D tensor,[Batch]\n",
    "        Returns\n",
    "        ------\n",
    "        outputs:2D tensor,[Batch, Hidden]\n",
    "        \"\"\"\n",
    "        batch_size, max_length, dim = keys.size()\n",
    "        query = query.unsqueeze(1).expand(-1, max_length, -1)\n",
    "        din_all = torch.cat([query, keys, query-keys, query*keys], dim=-1)\n",
    "        din_all = din_all.view(batch_size * max_length, -1)\n",
    "#         print('din_all:', din_all.shape)\n",
    "        outputs = self.mlp(din_all)\n",
    "        outputs = self.fc(outputs).view(batch_size, max_length) #[B, T]\n",
    "        \n",
    "        #scale\n",
    "        outputs = outputs / (dim ** 0.5)\n",
    "        #Mask\n",
    "        mask = (torch.arange(max_length, device=keys_length.device).repeat(batch_size, 1)<keys_length.view(-1,1))\n",
    "        outputs[~mask] = -np.inf\n",
    "        #Activation\n",
    "        outputs = F.softmax(outputs, dim=1) #din uses sigmod,dien uses softmax,[B,T]\n",
    "        \n",
    "        if not self.return_scores:\n",
    "            #weighted sum\n",
    "            outputs = torch.matmul(outputs.unsqueeze(1), keys).squeeze() #[B,H]\n",
    "\n",
    "        return outputs\n",
    "#用来得到兴趣抽取层每步隐藏状态的辅助网络\n",
    "class AuxiliaryNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, activation='sigmoid'):\n",
    "        super().__init__()\n",
    "        modules = OrderedDict()\n",
    "        previous_size = input_size\n",
    "        for index, hidden_layer in enumerate(hidden_layers):\n",
    "            modules[f\"dense{index}\"] = nn.Linear(previous_size, hidden_layer)\n",
    "            if activation:\n",
    "                modules[f\"activation{index}\"] = get_activation_layer(activation)\n",
    "            previous_size = hidden_layer\n",
    "        modules['final_layer'] = nn.Linear(previous_size, 1)\n",
    "        self.mlp = nn.Sequential(modules)\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.mlp(x))\n",
    "#兴趣提取层，该层调用了兴趣演化层的gru得到了最终的用户演化兴趣\n",
    "class Interest(nn.Module):\n",
    "    SUPPORTED_GRU_TYPE = ['GRU', 'AIGRU', 'AGRU', 'AUGRU']\n",
    "    \n",
    "    def __init__(self, input_size, gru_type='AUGRU', gru_dropout=0.0, att_hidden_layers=[80,40], att_dropout=0.0, att_batchnorm=True, \n",
    "                att_activation='prelu', use_negsampling=False):\n",
    "        super(Interest, self).__init__()\n",
    "        if gru_type not in Interest.SUPPORTED_GRU_TYPE:\n",
    "            raise NotImplementedError(f\"gru_type:{gru_type} is not supported\")\n",
    "        self.gru_type = gru_type\n",
    "        self.use_negsampling = use_negsampling\n",
    "        self.interest_extractor = nn.GRU(input_size=input_size, hidden_size=input_size, batch_first=True, bidirectional=False)\n",
    "        \n",
    "        if self.use_negsampling:\n",
    "            self.auxiliary_net = AuxiliaryNet(input_size * 2, hidden_layers=[100,50])\n",
    "        if gru_type == 'GRU':\n",
    "            self.attention = Attention(input_size=input_size, hidden_layers=att_hidden_layers, dropout=att_dropout,\n",
    "                                      batchnorm=att_batchnorm, activation=att_activation)\n",
    "            self.interest_evolution = nn.GRU(input_size=input_size, hidden_size=input_size, batch_first=True, bidirectional=False)\n",
    "        elif gru_type == 'AIGRU':\n",
    "            self.attention = Attention(input_size=input_size, hidden_layers=att_hidden_layers, dropout=att_dropout, batchnorm=att_batchnorm,\n",
    "                                      activation=att_activation,return_scores=True)\n",
    "            self.interest_evolution = nn.GRU(inputt_size=input_size, hidden_size=input_size, batch_first=True, bidirectional=False)\n",
    "        elif gru_type == 'AGRU' or gru_type == 'AUGRU':\n",
    "            self.attention = Attention(input_size=input_size, hidden_layers=att_hidden_layers, dropout=att_dropout, batchnorm=att_batchnorm,\n",
    "                                      activation=att_activation, return_scores=True)\n",
    "            self.interest_evolution = DynamicGRU(input_size=input_size, hidden_size=input_size, gru_type=gru_type)\n",
    "    @staticmethod\n",
    "    def get_last_state(states, keys_length):\n",
    "        #states[B,T,H]\n",
    "        batch_size, max_seq_length, hidden_size = states.size()\n",
    "        mask = (torch.arange(max_seq_length, device=keys_length.device).repeat(batch_size,1)==(keys_length.view(-1,1)-1))\n",
    "        return states[mask]\n",
    "    def cal_auxiliary_loss(self, states, click_seq, noclick_seq, keys_length):\n",
    "        #states [B,T,H],兴趣提取层隐藏状态的输出\n",
    "        #click_seq [B,T,H]，下一个时刻用户点击的embedding向量\n",
    "        #noclick_seq [B,T,H] ，下一个时刻用户未点击的embedding向量\n",
    "        #keys_length [B]   用户历史行为序列的长度，注意这里是原序列长度-1，因为最后一个时间步的输出就没法计算了\n",
    "        batch_size, max_seq_length, embedding_size = states.size()\n",
    "        mask = (torch.arange(max_seq_length, device=states.device).repeat(batch_size,1) < keys_length.view(-1,1)).float()\n",
    "        click_input = torch.cat([states, click_seq], dim=-1)\n",
    "        noclick_input = torch.cat([states,noclick_seq], dim=-1)\n",
    "        embedding_size = embedding_size * 2\n",
    "        click_p = self.auxiliary_net(click_input.view(batch_size * max_seq_length, embedding_size)).view(batch_size, max_seq_length)[mask > 0].view(-1,1)\n",
    "        click_target = torch.ones(click_p.size(), dtype=torch.float, device=click_p.device)\n",
    "        noclick_p = self.auxiliary_net(noclick_input.view(batch_size * max_seq_length, embedding_size)).view(batch_size, max_seq_length)[mask>0].view(-1,1)\n",
    "        noclick_target = torch.zeros(noclick_p.size(), dtype=torch.float, device=noclick_p.device)\n",
    "        \n",
    "        loss = F.binary_cross_entropy(torch.cat([click_p, noclick_p],dim=0),\n",
    "                                     torch.cat([click_target, noclick_target], dim=0))\n",
    "        return loss\n",
    "    def forward(self, query, keys, keys_length, neg_keys=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ------------\n",
    "        query:2D tensor,[Batch, Hidden]\n",
    "        keys:3D tensor,[Batch, Time, Hidden]\n",
    "        keys_length:1D tensor,[Batch]\n",
    "        neg_keys:3D tensor,[Batch,Time, Hidden]\n",
    "        \n",
    "        returns\n",
    "        outputs:2D tensor, [Batch, Hidden]\n",
    "        \"\"\"\n",
    "        batch_size, max_length, dim = keys.size()\n",
    "        packed_keys = pack_padded_sequence(keys, lengths=keys_length.squeeze().cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_interests, _ = self.interest_extractor(packed_keys)\n",
    "        aloss = None\n",
    "        if (self.gru_type != 'GRU') or self.use_negsampling:\n",
    "            interests, _ = pad_packed_sequence(packed_interests, batch_first=True, padding_value=0.0, total_length=max_length)\n",
    "            if self.use_negsampling:\n",
    "                aloss = self.cal_auxiliary_loss(interests[:,:-1,:], keys[:,1:,:], neg_keys[:,1:,:], keys_length-1)\n",
    "#                 print('aloss:', aloss)\n",
    "        if self.gru_type == 'GRU':\n",
    "            packed_interests, _ = self.interest_evolution(packed_interests)\n",
    "            interests, _ = pad_packed_sequence(packed_interests, batch_first=True, padding_value=0.0, total_length=max_length)\n",
    "            outputs = self.attention(query, interests, keys_length)\n",
    "        elif self.gru_type == 'AIGRU':\n",
    "            #attention，直接在兴趣提取层的输出向量上乘以注意力得分\n",
    "            scores = self.attention(query, interests, keys_length)\n",
    "            interests = interests * scores.unsqueeze(-1)\n",
    "            \n",
    "            packed_interests = pack_padded_sequence(interests, lengths=keys_length.squeeze().cpu(), batch_first=True, enforce_sorted=False)\n",
    "            _, outputs = self.interest_evolution(packed_interests)\n",
    "            outputs = outputs.squeeze()\n",
    "        elif self.gru_type == 'AGRU' or self.gru_type == 'AUGRU':\n",
    "            #attention,得到注意力得分\n",
    "            scores = self.attention(query, interests, keys_length)\n",
    "#             print('scores:', scores)\n",
    "            packed_interests = pack_padded_sequence(interests, lengths=keys_length.squeeze().cpu(), batch_first=True, enforce_sorted=False)\n",
    "            packed_scores = pack_padded_sequence(scores, lengths=keys_length.squeeze().cpu(), batch_first=True, enforce_sorted=False)\n",
    "            outputs, _ = pad_packed_sequence(self.interest_evolution(packed_interests, packed_scores), batch_first=True)\n",
    "            #pick last state\n",
    "            outputs = Interest.get_last_state(outputs, keys_length.squeeze())\n",
    "            \n",
    "        return outputs, aloss\n",
    "class AttentionGroup(object):\n",
    "    def __init__(self, name, pairs, hidden_layers, activation='dice', att_dropout=0.0, gru_type='AUGRU', gru_dropout=0.0):\n",
    "        self.name = name\n",
    "        self.pairs = pairs\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.activation = activation\n",
    "        self.att_dropout = att_dropout\n",
    "        self.gru_type = gru_type\n",
    "        self.gru_dropout = gru_dropout\n",
    "        \n",
    "        self.related_feature_names = set()\n",
    "        self.neg_feature_names = set()\n",
    "        for pair in pairs:\n",
    "            self.related_feature_names.add(pair['ad'])\n",
    "            self.related_feature_names.add(pair['pos_hist'])\n",
    "            if 'neg_hist' in pair:\n",
    "                self.related_feature_names.add(pair['neg_hist'])\n",
    "                self.neg_feature_names.add(pair['neg_hist'])\n",
    "    def is_attention_feature(self, feature_name):\n",
    "        if feature_name in self.related_feature_names:\n",
    "            return True\n",
    "        return False\n",
    "    def is_neg_sampling_feature(self, feature_name):\n",
    "        if feature_name in self.neg_feature_names:\n",
    "            return True\n",
    "        return False\n",
    "    @property\n",
    "    def pairs_count(self):\n",
    "        return len(self.pairs)\n",
    "class DIEN(nn.Module):\n",
    "    def __init__(self, num_features, cat_features, seq_features, cat_nums, embedding_size, attention_groups, mlp_hidden_layers, mlp_activation='prelu',\n",
    "                mlp_dropout=0.0, use_negsampling=False, d_out=1):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.cat_features = cat_features\n",
    "        self.seq_features = seq_features\n",
    "        self.cat_nums = cat_nums\n",
    "        self.embedding_size = embedding_size\n",
    "        self.attention_groups = attention_groups\n",
    "        self.mlp_hidden_layers = mlp_hidden_layers\n",
    "        self.mlp_activation = mlp_activation\n",
    "        self.mlp_dropout = mlp_dropout\n",
    "        \n",
    "        self.d_out = d_out\n",
    "        self.use_negsampling = use_negsampling\n",
    "        #embedding\n",
    "        self.embeddings = OrderedDict()\n",
    "        #对于普通类别特征和序列特征都做embedding\n",
    "        for feature in self.cat_features + self.seq_features:\n",
    "            self.embeddings[feature] = nn.Embedding(self.cat_nums[feature], self.embedding_size, padding_idx=0)\n",
    "            self.add_module(f\"embedding:{feature}\", self.embeddings[feature])\n",
    "        self.sequence_poolings = OrderedDict()\n",
    "        self.attention_poolings = OrderedDict()\n",
    "        total_embedding_sizes = 0\n",
    "        for feature in self.cat_features:\n",
    "            total_embedding_sizes += self.embedding_size\n",
    "        for feature in self.seq_features:\n",
    "            if not self.is_neg_sampling_feature(feature):\n",
    "                total_embedding_sizes += self.embedding_size\n",
    "        \n",
    "        #sequence_pooling\n",
    "        for feature in self.seq_features:\n",
    "            if not self.is_attention_feature(feature):\n",
    "                self.sequence_poolings[feature] = MaxPooling(1)\n",
    "                self.add_module(f\"pooling:{feature}\", self.sequence_poolings[feature])\n",
    "        #attention_pooling\n",
    "        for attention_group in self.attention_groups:\n",
    "            self.attention_poolings[attention_group.name] = self.create_attention_fn(attention_group)\n",
    "            self.add_module(f\"attention_pooling:{attention_group.name}\", self.attention_poolings[attention_group.name])\n",
    "        total_input_size = total_embedding_sizes + len(self.num_features)\n",
    "        \n",
    "        self.mlp = MLP(total_input_size, mlp_hidden_layers, dropout=mlp_dropout, batchnorm=True, activation=mlp_activation)\n",
    "        self.final_layer = nn.Linear(mlp_hidden_layers[-1], self.d_out)\n",
    "        self.apply(init_weights)\n",
    "    def forward(self, x):\n",
    "        final_layer_inputs = list()\n",
    "        #linear\n",
    "        number_inputs = list()\n",
    "        for feature in self.num_features:\n",
    "            number_inputs.append(x[feature].view(-1,1))\n",
    "        \n",
    "        embeddings = OrderedDict()\n",
    "        #对普通类别特征做embedding\n",
    "        for feature in self.cat_features:\n",
    "            embeddings[feature] = self.embeddings[feature](x[feature].long())\n",
    "        #如果序列特征不做注意力，那就用sequence_pooling\n",
    "        for feature in self.seq_features:\n",
    "            if not self.is_attention_feature(feature):\n",
    "                embeddings[feature] = self.sequence_poolings[feature](self.embeddings[feature](x[feature].long()))\n",
    "        auxiliary_losses = []\n",
    "        for attention_group in self.attention_groups:\n",
    "            query = torch.cat([embeddings[pair['ad']] for pair in attention_group.pairs], dim=-1)\n",
    "            pos_hist = torch.cat([self.embeddings[pair['pos_hist']](x[pair['pos_hist']].long()) for pair in attention_group.pairs], dim=-1)\n",
    "\n",
    "            keys_length = torch.min(torch.cat([torch.sum(x[pair['pos_hist']]>0,dim=1).view(-1,1) for pair in attention_group.pairs],dim=-1), dim=-1)[0]\n",
    "            neg_hist = None\n",
    "            if self.use_negsampling:\n",
    "                neg_hist = torch.cat([self.embeddings[pair['neg_hist']](x[pair['neg_hist']].long()) for pair in attention_group.pairs], dim=-1)\n",
    "            embeddings[attention_group.name], tmp_loss = self.attention_poolings[attention_group.name](query, pos_hist, keys_length, neg_hist)\n",
    "            if tmp_loss is not None:\n",
    "                auxiliary_losses.append(tmp_loss)\n",
    "        emb_concat = torch.cat(number_inputs + [emb for emb in embeddings.values()], dim=-1)\n",
    "#         print('emb_concat:', emb_concat.shape)\n",
    "        final_layer_inputs = self.mlp(emb_concat)\n",
    "        output = self.final_layer(final_layer_inputs)\n",
    "        \n",
    "        auxiliary_avg_loss = None\n",
    "        if auxiliary_losses:\n",
    "            auxiliary_avg_loss = auxiliary_losses[0]\n",
    "            size = len(auxiliary_losses)\n",
    "            for i in range(1, size):\n",
    "                auxiliary_avg_loss += auxiliary_losses[i]\n",
    "            auxiliary_avg_loss /= size\n",
    "        if self.d_out == 1:\n",
    "            output = output.squeeze()\n",
    "            output = torch.sigmoid(output)\n",
    "        return output, auxiliary_avg_loss\n",
    "    def create_attention_fn(self, attention_group):\n",
    "        return Interest(attention_group.pairs_count * self.embedding_size, gru_type=attention_group.gru_type,\n",
    "                       gru_dropout= attention_group.gru_dropout, att_hidden_layers=attention_group.hidden_layers,\n",
    "                       att_dropout=attention_group.att_dropout,att_activation=attention_group.activation,\n",
    "                       use_negsampling=self.use_negsampling)\n",
    "    def is_attention_feature(self, feature):\n",
    "        for group in self.attention_groups:\n",
    "            if group.is_attention_feature(feature):\n",
    "                return True\n",
    "        return False\n",
    "    def is_neg_sampling_feature(self, feature):\n",
    "        for group in self.attention_groups:\n",
    "            if group.is_neg_sampling_feature(feature):\n",
    "                return True\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DIEN(\n",
       "  (embedding:gender): Embedding(4, 16, padding_idx=0)\n",
       "  (embedding:movieId): Embedding(280, 16, padding_idx=0)\n",
       "  (embedding:occupation): Embedding(23, 16, padding_idx=0)\n",
       "  (embedding:zipCode): Embedding(124, 16, padding_idx=0)\n",
       "  (embedding:genres): Embedding(20, 16, padding_idx=0)\n",
       "  (embedding:histHighRatedMovieIds): Embedding(1791, 16, padding_idx=0)\n",
       "  (embedding:negHistMovieIds): Embedding(3868, 16, padding_idx=0)\n",
       "  (pooling:genres): MaxPooling()\n",
       "  (attention_pooling:group1): Interest(\n",
       "    (interest_extractor): GRU(16, 16, batch_first=True)\n",
       "    (auxiliary_net): AuxiliaryNet(\n",
       "      (mlp): Sequential(\n",
       "        (dense0): Linear(in_features=32, out_features=100, bias=True)\n",
       "        (activation0): Sigmoid()\n",
       "        (dense1): Linear(in_features=100, out_features=50, bias=True)\n",
       "        (activation1): Sigmoid()\n",
       "        (final_layer): Linear(in_features=50, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (attention): Attention(\n",
       "      (mlp): MLP(\n",
       "        (mlp): Sequential(\n",
       "          (dense0): Linear(in_features=64, out_features=16, bias=True)\n",
       "          (batchnorm0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation0): Dice(\n",
       "            (bn): BatchNorm1d(16, eps=1e-08, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (dropout0): Dropout(p=0.1)\n",
       "          (dense1): Linear(in_features=16, out_features=8, bias=True)\n",
       "          (batchnorm1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation1): Dice(\n",
       "            (bn): BatchNorm1d(8, eps=1e-08, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=8, out_features=1, bias=True)\n",
       "    )\n",
       "    (interest_evolution): DynamicGRU(\n",
       "      (rnn): AttentionUpdateGateGRUCell()\n",
       "    )\n",
       "  )\n",
       "  (mlp): MLP(\n",
       "    (mlp): Sequential(\n",
       "      (dense0): Linear(in_features=97, out_features=32, bias=True)\n",
       "      (batchnorm0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation0): PReLU(num_parameters=1)\n",
       "      (dropout0): Dropout(p=0.25)\n",
       "      (dense1): Linear(in_features=32, out_features=16, bias=True)\n",
       "      (batchnorm1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation1): PReLU(num_parameters=1)\n",
       "      (dropout1): Dropout(p=0.25)\n",
       "    )\n",
       "  )\n",
       "  (final_layer): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_net():\n",
    "    augru_attention_groups_with_neg = [\n",
    "    AttentionGroup(\n",
    "        name='group1',\n",
    "        pairs=[{'ad': 'movieId', 'pos_hist': 'histHighRatedMovieIds', 'neg_hist': 'negHistMovieIds'}],\n",
    "        hidden_layers=[16, 8], att_dropout=0.1, gru_type='AUGRU')\n",
    "    ]\n",
    " \n",
    "    net = DIEN(num_features=num_features,\n",
    "           cat_features=cat_features,\n",
    "           seq_features=seq_features,\n",
    "           cat_nums = cat_nums,\n",
    "           embedding_size=16,\n",
    "           attention_groups=augru_attention_groups_with_neg,\n",
    "           mlp_hidden_layers=[32,16],\n",
    "           mlp_activation=\"prelu\",\n",
    "           mlp_dropout=0.25,\n",
    "           use_negsampling=True,\n",
    "           d_out=1\n",
    "           )\n",
    "    \n",
    "    return net \n",
    " \n",
    "net = create_net() \n",
    "net\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8949, 0.8928, 0.9280, 0.4070, 0.9930, 0.9201, 0.8962, 0.9338, 0.9414,\n",
      "        0.7865, 0.9624, 0.6806, 0.7249, 0.8036, 0.9842, 0.8924, 0.9777, 0.8608,\n",
      "        0.7367, 0.9364, 0.9596, 0.8733, 0.5051, 0.9635, 0.9704, 0.8239, 0.9314,\n",
      "        0.9130, 0.5094, 0.9463, 0.9574, 0.9567, 0.7361, 0.9387, 0.8886, 0.9779,\n",
      "        0.7662, 0.9182, 0.5474, 0.8799, 0.4164, 0.7268, 0.9168, 0.7534, 0.9164,\n",
      "        0.6439, 0.9880, 0.9688, 0.8415, 0.9474, 0.9217, 0.9093, 0.8461, 0.8523,\n",
      "        0.7598, 0.7161, 0.8888, 0.9086, 0.9788, 0.9656, 0.8674, 0.9881, 0.3559,\n",
      "        0.9080, 0.8879, 0.9797, 0.8703, 0.8856, 0.9551, 0.8666, 0.6848, 0.4805,\n",
      "        0.8486, 0.6566, 0.9640, 0.9680, 0.9112, 0.5340, 0.8245, 0.9276, 0.5719,\n",
      "        0.7532, 0.9154, 0.9146, 0.2682, 0.4799, 0.9410, 0.8417, 0.9239, 0.9349,\n",
      "        0.9812, 0.9834, 0.8919, 0.9012, 0.8113, 0.9555, 0.8512, 0.6907, 0.7667,\n",
      "        0.8759, 0.1544, 0.8564, 0.8684, 0.9537, 0.8318, 0.8043, 0.9726, 0.5908,\n",
      "        0.6833, 0.7921, 0.7578, 0.9559, 0.5540, 0.8013, 0.9376, 0.8275, 0.5877,\n",
      "        0.9519, 0.9099, 0.9537, 0.7768, 0.8744, 0.8765, 0.7411, 0.9212, 0.5166,\n",
      "        0.9661, 0.7334], grad_fn=<SigmoidBackward>)\n",
      "torch.Size([128])\n",
      "tensor(1.3627, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out,aloss = net.forward(batch)\n",
    "print(out)\n",
    "print(out.shape)\n",
    "print(aloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def auc(y_pred, y_true):\n",
    "    pred = y_pred.data\n",
    "    y = y_true.data\n",
    "    return roc_auc_score(y, pred)\n",
    "\n",
    "loss_func = nn.BCELoss()\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=0.001)\n",
    "metric_func = auc\n",
    "metric_name = 'auc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_training.........\n",
      "================================================================2022-08-17 11:00:40\n",
      "[step=10] loss: 1.556, auc: 0.553\n",
      "[step=20] loss: 1.544, auc: 0.556\n",
      "[step=30] loss: 1.522, auc: 0.562\n",
      "\n",
      "EPOCH=1, loss=1.506, auc = 0.569, val_loss=1.435, val_auc = 0.590\n",
      "\n",
      "================================================================================2022-08-17 11:00:44\n",
      "[step=10] loss: 1.463, auc: 0.582\n",
      "[step=20] loss: 1.456, auc: 0.571\n",
      "[step=30] loss: 1.440, auc: 0.580\n",
      "\n",
      "EPOCH=2, loss=1.439, auc = 0.576, val_loss=1.379, val_auc = 0.596\n",
      "\n",
      "================================================================================2022-08-17 11:00:47\n",
      "[step=10] loss: 1.439, auc: 0.532\n",
      "[step=20] loss: 1.415, auc: 0.567\n",
      "[step=30] loss: 1.400, auc: 0.574\n",
      "\n",
      "EPOCH=3, loss=1.399, auc = 0.569, val_loss=1.326, val_auc = 0.601\n",
      "\n",
      "================================================================================2022-08-17 11:00:50\n",
      "[step=10] loss: 1.333, auc: 0.607\n",
      "[step=20] loss: 1.333, auc: 0.594\n",
      "[step=30] loss: 1.329, auc: 0.592\n",
      "\n",
      "EPOCH=4, loss=1.326, auc = 0.588, val_loss=1.264, val_auc = 0.604\n",
      "\n",
      "================================================================================2022-08-17 11:00:54\n",
      "[step=10] loss: 1.283, auc: 0.563\n",
      "[step=20] loss: 1.274, auc: 0.579\n",
      "[step=30] loss: 1.260, auc: 0.592\n",
      "\n",
      "EPOCH=5, loss=1.257, auc = 0.588, val_loss=1.191, val_auc = 0.603\n",
      "\n",
      "================================================================================2022-08-17 11:00:57\n",
      "[step=10] loss: 1.178, auc: 0.608\n",
      "[step=20] loss: 1.174, auc: 0.613\n",
      "[step=30] loss: 1.169, auc: 0.612\n",
      "\n",
      "EPOCH=6, loss=1.160, auc = 0.613, val_loss=1.113, val_auc = 0.606\n",
      "\n",
      "================================================================================2022-08-17 11:01:00\n",
      "[step=10] loss: 1.085, auc: 0.624\n",
      "[step=20] loss: 1.086, auc: 0.618\n",
      "[step=30] loss: 1.087, auc: 0.610\n",
      "\n",
      "EPOCH=7, loss=1.080, auc = 0.612, val_loss=1.037, val_auc = 0.607\n",
      "\n",
      "================================================================================2022-08-17 11:01:03\n",
      "[step=10] loss: 1.010, auc: 0.626\n",
      "[step=20] loss: 1.020, auc: 0.608\n",
      "[step=30] loss: 1.014, auc: 0.608\n",
      "\n",
      "EPOCH=8, loss=1.008, auc = 0.610, val_loss=0.973, val_auc = 0.606\n",
      "\n",
      "================================================================================2022-08-17 11:01:07\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "epochs = 8\n",
    "log_step_freq = 10\n",
    "\n",
    "dfhistory = pd.DataFrame(columns=['epoch', 'loss', metric_name, 'val_loss', 'val_'+metric_name])\n",
    "\n",
    "print('start_training.........')\n",
    "nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "print('========'*8 + '%s' %nowtime)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    \n",
    "    # 训练阶段\n",
    "    net.train()\n",
    "    loss_sum = 0.0\n",
    "    metric_sum = 0.0\n",
    "    step = 1\n",
    "    \n",
    "    for step, batch in enumerate(dl_train, 1):\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 正向传播\n",
    "        predictions, aloss = net(batch)\n",
    "        labels = batch['label']\n",
    "        loss = loss_func(predictions, labels) + aloss\n",
    "        try:\n",
    "            metric = metric_func(predictions, labels)\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 打印batch级别日志\n",
    "        loss_sum += loss.item()\n",
    "        metric_sum += metric.item()\n",
    "        if step % log_step_freq == 0:\n",
    "            print((\"[step=%d] loss: %.3f, \" + metric_name + \": %.3f\") % (step, loss_sum/step, metric_sum/step));\n",
    "    \n",
    "    # 验证阶段\n",
    "    net.eval()\n",
    "    val_loss_sum = 0.0\n",
    "    val_metric_sum = 0.0\n",
    "    val_step = 1\n",
    "    \n",
    "    for val_step, batch in enumerate(df_val, 1):\n",
    "        with torch.no_grad():\n",
    "            predictions, aloss = net(batch)\n",
    "            labels = batch['label']\n",
    "            val_loss = loss_func(predictions, labels) + aloss\n",
    "            try:\n",
    "                val_metric = metric_func(predictions, labels)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        val_loss_sum += val_loss.item()\n",
    "        val_metric_sum += val_metric.item()\n",
    "    \n",
    "    # 记录日志\n",
    "    info = (epoch, loss_sum/step, metric_sum/step, val_loss_sum/val_step, val_metric_sum/val_step)\n",
    "    dfhistory.loc[epoch-1] = info\n",
    "    \n",
    "    # 打印日志\n",
    "    print((\"\\nEPOCH=%d, loss=%.3f, \" + metric_name + \" = %.3f, val_loss=%.3f, \" + \"val_\" + metric_name + \" = %.3f\") %info)\n",
    "    nowtime = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print('\\n' + '=========='* 8 + '%s' %nowtime)\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_metric(dfhistory, metric):\n",
    "    train_metrics = dfhistory[metric]\n",
    "    val_metrics = dfhistory['val_'+metric]\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "    plt.plot(epochs, train_metrics, 'bo--')\n",
    "    plt.plot(epochs, val_metrics, 'ro-')\n",
    "    plt.title('Training and validation '+ metric)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([\"train_\"+metric, 'val_'+metric])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 观察损失和准确率的变化\n",
    "plot_metric(dfhistory,\"loss\")\n",
    "plot_metric(dfhistory,\"auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_auc:0.611, val_acc:0.612\n"
     ]
    }
   ],
   "source": [
    "# 预测\n",
    "from sklearn.metrics import accuracy_score\n",
    "labels = dfval['label'].values\n",
    "y_pred_probs = torch.cat([net(x)[0] for x in df_val]) \n",
    "y_pred = torch.where(y_pred_probs>0.5, torch.ones_like(y_pred_probs), torch.zeros_like(y_pred_probs))\n",
    "val_auc = auc(y_pred_probs.data.numpy(),labels)\n",
    "val_acc = accuracy_score(labels, y_pred.data.numpy())\n",
    "print('val_auc:%.3f, val_acc:%.3f' % (val_auc, val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
