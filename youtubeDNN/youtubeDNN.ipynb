{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c0b9cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad3f0be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepctr.feature_column import build_input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d6d9dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepctr.layers import DNN\n",
    "from deepctr.layers.utils import NoMask, combined_dnn_input\n",
    "from tensorflow.python.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e63cd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "303f8863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepctr.feature_column import SparseFeat, VarLenSparseFeat, create_embedding_matrix, embedding_lookup,get_dense_input, varlen_embedding_lookup, get_varlen_pooling_list, mergeDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47977c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_from_feature_columns(features, feature_columns, l2_reg, seed, prefix='', seq_mask_zero=True, \n",
    "                              support_dense=True, support_group=False, embedding_matrix_dict=None):\n",
    "    sparse_feature_columns = list(filter(lambda x:isinstance(x, SparseFeat), feature_columns)) if feature_columns else []\n",
    "    varlen_sparse_feature_columns = list(filter(lambda x:isinstance(x, VarLenSparseFeat), feature_columns)) if feature_columns else []\n",
    "    if embedding_matrix_dict is None:\n",
    "        embedding_matrix_dict = create_embedding_matrix(feature_columns, l2_reg, seed, prefix=prefix, seq_mask_zero=seq_mask_zero)\n",
    "    \n",
    "    group_sparse_embedding_dict = embedding_lookup(embedding_matrix_dict, features, sparse_feature_columns)\n",
    "    dense_value_list = get_dense_input(features, feature_columns)\n",
    "    if not support_dense and len(dense_value_list) >0:\n",
    "        raise ValueError('DenseFeat is not supported in dnn_feature_columns')\n",
    "    \n",
    "    sequence_embed_dict = varlen_embedding_lookup(embedding_matrix_dict, features, varlen_sparse_feature_columns)\n",
    "    group_varlen_sparse_embedding_dict = get_varlen_pooling_list(sequence_embed_dict, features,\n",
    "                                                                varlen_sparse_feature_columns)\n",
    "    group_embedding_dict = mergeDict(group_sparse_embedding_dict, group_varlen_sparse_embedding_dict)\n",
    "    if not support_group:\n",
    "        group_embedding_dict = list(chain.from_iterable(group_embedding_dict.values()))\n",
    "    \n",
    "    return group_embedding_dict, dense_value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d78d6df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from deepctr.layers.utils import reduce_max, reduce_mean, reduce_sum, concat_func, div, softmax\n",
    "from tensorflow.python.keras.initializers import Zeros\n",
    "from tensorflow.python.keras.layers import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a921ee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoolingLayer(Layer):\n",
    "    def __init__(self, mode='mean', supports_masking=False, **kwargs):\n",
    "        if mode not in ['sum', 'mean', 'max']:\n",
    "            raise ValueError('mode must be sum or mean')\n",
    "        self.mode = mode\n",
    "        self.eps = tf.constant(1e-8, tf.float32)\n",
    "        super(PoolingLayer, self).__init__(**kwargs)\n",
    "        self.supports_masking = supports_masking\n",
    "    def build(self, input_shape):\n",
    "        super(PoolingLayer, self).build(input_shape)#be sure to call this somewhere\n",
    "    def call(self, seq_value_len_list, mask=None, **kwargs):\n",
    "        if not isinstance(seq_value_len_list, list):\n",
    "            seq_value_len_list = [seq_value_len_list]\n",
    "        if len(seq_value_len_list) == 1:\n",
    "            return seq_value_len_list[0]\n",
    "        expand_seq_value_len_list = list(map(lambda x:tf.expand_dims(x, axis=-1), seq_value_len_list))\n",
    "        a = concat_func(expand_seq_value_len_list)\n",
    "        if self.mode == 'mean':\n",
    "            hist = reduce_mean(a, axis=-1,)\n",
    "        if self.mode == 'sum':\n",
    "            hist = reduce_sum(a, axis=-1,)\n",
    "        if self.mode == 'max':\n",
    "            hist = reduce_max(a, axis=-1,)\n",
    "        return hist\n",
    "    def get_conif(self,):\n",
    "        config = {'mode':self.mode, 'supports_masking':self.supports_masking}\n",
    "        base_config = super(PoolingLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "354a4bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampledSoftmaxLayer(Layer):\n",
    "    def __init__(self, sampler_config, temperature=1.0, **kwargs):\n",
    "        self.sampler_config = sampler_config\n",
    "        self.temperature = temperature\n",
    "        self.sampler = self.sampler_config['sampler']\n",
    "        self.item_count = self.sampler_config['item_count']\n",
    "        \n",
    "        super(SampledSoftmaxLayer, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.vocabulary_size = input_shape[0][0]\n",
    "        self.zero_bias = self.add_weight(shape=[self.vocabulary_size], initializer=Zeros, dtype=tf.float32,\n",
    "                                        trainable=False, name='bias')\n",
    "        super(SampledSoftmaxLayer, self).build(input_shape)\n",
    "    def call(self, inputs_with_item_idx, training=None, **kwargs):\n",
    "        item_embeddings, user_vec, item_idx = inputs_with_item_idx\n",
    "        if item_idx.dtype != tf.int64:\n",
    "            item_idx = tf.cast(item_idx, tf.int64)\n",
    "        user_vec /= self.temperature\n",
    "        if self.sampler == 'inbatch':\n",
    "            item_vec = tf.gather(item_embeddings, tf.squeeze(item_idx, axis=1))\n",
    "            logits = tf.matmul(user_vec, item_vec, transpose_b=True)\n",
    "            loss = inbatch_softmax_cross_entropy_with_logits(logits, self.item_count, item_idx)\n",
    "        else:\n",
    "            num_sampled = self.sampler_config['num_sampled']\n",
    "            if self.sampler == 'frequency':\n",
    "                sampled_values = tf.nn.fixed_unigram_candidate_sampler(item_idx, 1, num_sampled, True,\n",
    "                                                                      self.vocabulary_size,\n",
    "                                                                      distortion=self.sampler_config['distortion'],\n",
    "                                                                      unigrams=np.maximum(self.item_count,1).tolist(),\n",
    "                                                                      seed=None, name=None)\n",
    "            elif self.sampler == 'adaptive':\n",
    "                sampled_values = tf.nn.learned_unigram_candidate_sampler(item_idx, 1, num_sampled, True,\n",
    "                                                                        self.vocabulary_size, seed=None, name=None)\n",
    "            elif self.sampler == 'uniform':\n",
    "                try:\n",
    "                    sampled_values = tf.nn.uniform_candidate_sampler(item_idx, 1, num_sampled, True,\n",
    "                                                                    self.vocabulary_size, seed=None, name=None)\n",
    "                except AttributeError:\n",
    "                    sampled_values = tf.random.uniform_candidate_sampler(item_idx, 1, num_sampled, True,\n",
    "                                                                        self.vocabulary_size, seed=None, name=None)\n",
    "            else:\n",
    "                raise ValueError('%s sampler is not supported ' % self.sampler)\n",
    "            \n",
    "            loss = tf.nn.sampled_softmax_loss(weights=item_embeddings, biases=self.zero_bias, labels=item_idx,\n",
    "                                             inputs=user_vec, num_sampled=num_sampled,\n",
    "                                             num_classes=self.vocabulary_size, sampled_values=sampled_values)\n",
    "        return tf.expand_dims(loss, axis=1)\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, 1)\n",
    "    def get_config(self,):\n",
    "        config = {'sampler_config':self.sampler_config, 'temperature':self.temperature}\n",
    "        base_config = super(SampledSoftmaxLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f8683ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingIndex(Layer):\n",
    "    def __init__(self, index, **kwargs):\n",
    "        self.index = index\n",
    "        super(EmbeddingIndex, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        super(EmbeddingIndex, self).build(input_shape)\n",
    "    def call(self, x, **kwargs):\n",
    "        return tf.constant(self.index)\n",
    "    def get_config(self,):\n",
    "        config = {'index':self.index,}\n",
    "        base_config = super(EmbeddingIndex, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a36af62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7fdde0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "class NegativeSampler(namedtuple('NegativeSampler', ['sampler', 'num_sampled', 'item_name', 'item_count', 'distortion'])):\n",
    "    \"\"\"\n",
    "    sampler:sampler name, ['inbatch', 'uniform', 'frequency', 'adaptive']\n",
    "    num_sampled:negative samples number per one positive sample\n",
    "    item_name:pkey of item features.\n",
    "    item_count:global frequency of item\n",
    "    distortion:skew factor of the unigram probability distribution\n",
    "    \"\"\"\n",
    "    __slots__ = ()\n",
    "    \n",
    "    def __new__(cls, sampler, num_sampled, item_name, item_count=None, distortion=1.0,):\n",
    "        if sampler not in ['inbatch', 'uniform', 'frequency', 'adaptive']:\n",
    "            raise ValueError('%s sampler is not supported ' % sampler)\n",
    "        if sampler in ['inbatch', 'frequency'] and item_count is None:\n",
    "            raise ValueError('item_count must not be None when using inbatch or frequency sampler')\n",
    "        return super(NegativeSampler, cls).__new__(cls, sampler, num_sampled, item_name, item_count, distortion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d79f1974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_normalize(x, axis=-1):\n",
    "    return Lambda(lambda x:tf.nn.l2_normalize(x, axis))(x)\n",
    "def get_item_embedding(item_embedding, item_input_layer):\n",
    "    return Lambda(lambda x:tf.squeeze(tf.gather(item_embedding, x), axis=1))(item_input_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f0694ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampledsoftmaxloss(y_true, y_pred):\n",
    "    return K.mean(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "be6d12b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def YoutubeDNN(user_feature_columns, item_feature_columns, user_dnn_hidden_units=(64,32), dnn_activation='relu',\n",
    "               dnn_use_bn=False, l2_reg_dnn=0, l2_reg_embedding=1e-6, dnn_dropout=0, output_activation='linear',\n",
    "               temperature=0.05, sampler_config=None, seed=1024):\n",
    "    \"\"\"\n",
    "    user_feature_columns: an iterable containing user's features used by the model\n",
    "    item_feature_columns: an iterable containing item's features used by the model\n",
    "    user_dnn_hidden_units: list, list of positive integer or empty list, the layer number and units in each layer of user tower\n",
    "    dnn_activation:activation function to use in deep net\n",
    "    dnn_use_bn:bool. whether use batchnormalization before activation or not in deep net\n",
    "    l2_reg_dnn:float. L2 regularizer strength applied to DNN\n",
    "    l2_reg_embedding:float. L2 regularizer strength applied to embedding vector\n",
    "    dnn_dropout:float in [0,1), the probability we will drop out a given DNN coordinate.\n",
    "    output_activation:Activation function to use in output layer\n",
    "    temperature:float. scaling factor\n",
    "    sampler_config:negative sample config\n",
    "    seed:integer. to use as random seed\n",
    "    return: A keras model instance.\n",
    "    \"\"\"\n",
    "    if len(item_feature_columns) > 1:\n",
    "        raise ValueError('Now YoutubeDNN only support 1 item feature like item_id')\n",
    "    item_feature_name = item_feature_columns[0].name\n",
    "    item_vocabulary_size = item_feature_columns[0].vocabulary_size\n",
    "    \n",
    "    embedding_matrix_dict = create_embedding_matrix(user_feature_columns + item_feature_columns, l2_reg_embedding,\n",
    "                                                   seed=seed)\n",
    "    user_features = build_input_features(user_feature_columns)\n",
    "    user_inputs_list = list(user_features.values())\n",
    "    user_sparse_embedding_list, user_dense_value_list = input_from_feature_columns(user_features, user_feature_columns,\n",
    "                                                                                  l2_reg_embedding, seed=seed,\n",
    "                                                                                  embedding_matrix_dict=embedding_matrix_dict)\n",
    "    user_dnn_input = combined_dnn_input(user_sparse_embedding_list, user_dense_value_list)\n",
    "    \n",
    "    item_features = build_input_features(item_feature_columns)\n",
    "    item_inputs_list = list(item_features.values())\n",
    "    user_dnn_out = DNN(user_dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn,\n",
    "                      output_activation=output_activation, seed=seed)(user_dnn_input)\n",
    "    user_dnn_out = l2_normalize(user_dnn_out)\n",
    "    \n",
    "    item_index = EmbeddingIndex(list(range(item_vocabulary_size)))(item_features[item_feature_name])\n",
    "    \n",
    "    item_embedding_matrix = embedding_matrix_dict[item_feature_name]\n",
    "    item_embedding_weight = NoMask()(item_embedding_matrix(item_index))\n",
    "    \n",
    "    pooling_item_embedding_weight = PoolingLayer()([item_embedding_weight])\n",
    "    \n",
    "    pooling_item_embedding_weight = l2_normalize(pooling_item_embedding_weight)\n",
    "    output = SampledSoftmaxLayer(sampler_config._asdict(),temperature)([pooling_item_embedding_weight, user_dnn_out,\n",
    "                                                                       item_features[item_feature_name]])\n",
    "    model = Model(inputs=user_inputs_list + item_inputs_list, outputs=output)\n",
    "    \n",
    "    model.__setattr__('user_input', user_inputs_list)\n",
    "    model.__setattr__('user_embedding', user_dnn_out)\n",
    "    \n",
    "    model.__setattr__('item_input', item_inputs_list)\n",
    "    model.__setattr__('item_embedding', get_item_embedding(pooling_item_embedding_weight, item_features[item_feature_name]))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd8bfbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "# from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "54b23cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data_set(data, seq_max_len=50, negsample=0):\n",
    "    data.sort_values('timestamp', inplace=True)\n",
    "    item_ids = data['movie_id'].unique()\n",
    "    item_id_genres_map = dict(zip(data['movie_id'].values, data['genres'].values))\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    for reviewerID, hist in tqdm(data.groupby('user_id')):\n",
    "        pos_list = hist['movie_id'].to_list()\n",
    "        genres_list = hist['genres'].tolist()\n",
    "        rating_list = hist['rating'].tolist()\n",
    "        \n",
    "        if negsample > 0:\n",
    "            candidate_set = list(set(item_ids) - set(pos_list))\n",
    "            neg_list = np.random.choice(candidate_set, size=len(pos_list)*negsample, replace=True)\n",
    "        for i in range(1, len(pos_list)):\n",
    "            hist = pos_list[:i]\n",
    "            genres_hist = genres_list[:i]\n",
    "            seq_len = min(i, seq_max_len)\n",
    "            if i != len(pos_list) - 1:\n",
    "                train_set.append((reviewerID, pos_list[i], 1, hist[::-1][:seq_len], seq_len, genres_hist[::-1][:seq_len],\n",
    "                                 genres_list[i], rating_list[i]))\n",
    "                for negi in range(negsample):\n",
    "                    train_set.append((reviewerID, neg_list[i * negsample + negi], 0, hist[::-1][:seq_len], seq_len,\n",
    "                                     genres_hist[::-1][:seq_len], item_id_genres_map[neg_list[i * negsample + negi]]))\n",
    "            else:\n",
    "                test_set.append((reviewerID, pos_list[i], 1, hist[::-1][:seq_len], seq_len, genres_hist[::-1][:seq_len],\n",
    "                                genres_list[i], rating_list[i]))\n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(test_set)\n",
    "    print(len(train_set[0]), len(test_set[0]))\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d208c32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_model_input(train_set, user_profile, seq_max_len):\n",
    "    train_uid = np.array([line[0] for line in train_set])\n",
    "    train_iid = np.array([line[1] for line in train_set])\n",
    "    train_label = np.array([line[2] for line in train_set])\n",
    "    train_seq = [line[3] for line in train_set]\n",
    "    train_hist_len = np.array([line[4] for line in train_set])\n",
    "    train_seq_genres = np.array([line[5] for line in train_set])\n",
    "    train_genres = np.array([line[6] for line in train_set])\n",
    "    train_seq_pad = pad_sequences(train_seq, maxlen=seq_max_len, padding='post', truncating='post', value=0)\n",
    "    train_seq_genres_pad = pad_sequences(train_seq_genres, maxlen=seq_max_len, padding='post', truncating='post',\n",
    "                                        value=0)\n",
    "    \n",
    "    train_model_input = {'user_id':train_uid, 'movie_id':train_iid, 'hist_movie_id':train_seq_pad,\n",
    "                        'hist_genres':train_seq_genres_pad, 'hist_len':train_hist_len, 'genres':train_genres}\n",
    "    \n",
    "    for key in ['gender', 'age', 'occupation', 'zip']:\n",
    "        train_model_input[key] = user_profile.loc[train_model_input['user_id']][key].values\n",
    "    \n",
    "    return train_model_input, train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd2d3b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from deepctr.feature_column import SparseFeat, VarLenSparseFeat\n",
    "# from deepmatch.models import *\n",
    "# from deepmatch.utils import sampledsoftmaxloss, NegativeSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0a5cc3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1496.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Trusfort\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 227 samples\n",
      "227/227 [==============================] - ETA: 0s - loss: 9.294 - 3s 15ms/sample - loss: 9.2943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2455: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 16)\n",
      "(208, 16)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    data = pd.read_csv('./movielens_sample.txt')\n",
    "    data['genres'] = list(map(lambda x:x.split('|')[0], data['genres'].values))\n",
    "    \n",
    "    sparse_features = ['movie_id', 'user_id', 'gender', 'age', 'occupation', 'zip', 'genres']\n",
    "    SEQ_LEN = 50\n",
    "    #1.Label Encoding for sparse features, and process sequence features with 'gen_data_set' and 'gen_model_input'\n",
    "    feature_max_idx = {}\n",
    "    for feature in sparse_features:\n",
    "        lbe = LabelEncoder()\n",
    "        data[feature] = lbe.fit_transform(data[feature]) + 1\n",
    "        feature_max_idx[feature] = data[feature].max() + 1\n",
    "    \n",
    "    user_profile = data[['user_id', 'gender', 'age', 'occupation', 'zip']].drop_duplicates('user_id')\n",
    "    \n",
    "    item_profile = data[['movie_id']].drop_duplicates('movie_id')\n",
    "    \n",
    "    user_profile.set_index('user_id', inplace=True)\n",
    "    user_item_list = data.groupby('user_id')['movie_id'].apply(list)\n",
    "    \n",
    "    train_set, test_set = gen_data_set(data, SEQ_LEN, 0)\n",
    "    \n",
    "    train_model_input, train_label = gen_model_input(train_set, user_profile, SEQ_LEN)\n",
    "    test_model_input, test_label = gen_model_input(test_set, user_profile, SEQ_LEN)\n",
    "    \n",
    "    #2. count #unique features for each sparse field and generate feature config for sequence feature\n",
    "    embedding_dim = 16\n",
    "    user_feature_columns = [SparseFeat('user_id', feature_max_idx['user_id'], embedding_dim),\n",
    "                           SparseFeat('gender', feature_max_idx['gender'], embedding_dim),\n",
    "                           SparseFeat('age', feature_max_idx['age'], embedding_dim),\n",
    "                           SparseFeat('occupation', feature_max_idx['occupation'], embedding_dim),\n",
    "                           SparseFeat('zip', feature_max_idx['zip'], embedding_dim),\n",
    "                           VarLenSparseFeat(SparseFeat('hist_movie_id', feature_max_idx['movie_id'], embedding_dim,\n",
    "                                                      embedding_name='movie_id'), SEQ_LEN, 'mean', 'hist_len'),\n",
    "                           VarLenSparseFeat(SparseFeat('hist_genres', feature_max_idx['genres'], embedding_dim,\n",
    "                                                      embedding_name='genres'), SEQ_LEN, 'mean', 'hist_len')\n",
    "                           ]\n",
    "    item_feature_columns = [SparseFeat('movie_id', feature_max_idx['movie_id'], embedding_dim)]\n",
    "    \n",
    "    from collections import Counter\n",
    "    \n",
    "    train_counter = Counter(train_model_input['movie_id'])\n",
    "    item_count = [train_counter.get(i, 0) for i in range(item_feature_columns[0].vocabulary_size)]\n",
    "    sampler_config = NegativeSampler('frequency', num_sampled=5, item_name='movie_id', item_count=item_count)\n",
    "    \n",
    "    #3.Define Model and train\n",
    "    import tensorflow as tf\n",
    "    if tf.__version__ >= '2.0.0':\n",
    "        tf.compat.v1.disable_eager_execution()\n",
    "    else:\n",
    "        K.set_learning_phase(True)\n",
    "        \n",
    "    model = YoutubeDNN(user_feature_columns, item_feature_columns, user_dnn_hidden_units=(64, embedding_dim),\n",
    "                      sampler_config=sampler_config)\n",
    "    model.compile(optimizer='adam', loss=sampledsoftmaxloss)\n",
    "    \n",
    "    history = model.fit(train_model_input, train_label, batch_size=256, epochs=1, verbose=1, validation_split=0.0,)\n",
    "    \n",
    "    #4. generate user features for testing and full item features for retrieval\n",
    "    test_user_model_input = test_model_input\n",
    "    all_item_model_input = {'movie_id':item_profile['movie_id'].values}\n",
    "    \n",
    "    user_embedding_model = Model(inputs=model.user_input, outputs=model.user_embedding)\n",
    "    item_embedding_model = Model(inputs=model.item_input, outputs=model.item_embedding)\n",
    "    \n",
    "    user_embs = user_embedding_model.predict(test_user_model_input, batch_size=2 ** 12)\n",
    "    item_embs = item_embedding_model.predict(all_item_model_input, batch_size=2 ** 12)\n",
    "    \n",
    "    print(user_embs.shape)\n",
    "    print(item_embs.shape)\n",
    "    \n",
    "    #5. [Optional] ANN search by faiss and evaulate the result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c794e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15f4b4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "555b8b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "127faf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./movielens_sample.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a1de930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "      <td>One Flew Over the Cuckoo's Nest (1975)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "      <td>James and the Giant Peach (1996)</td>\n",
       "      <td>Animation|Children's|Musical</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "      <td>My Fair Lady (1964)</td>\n",
       "      <td>Musical|Romance</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "      <td>Erin Brockovich (2000)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "      <td>Bug's Life, A (1998)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating  timestamp  \\\n",
       "0        1      1193       5  978300760   \n",
       "1        1       661       3  978302109   \n",
       "2        1       914       3  978301968   \n",
       "3        1      3408       4  978300275   \n",
       "4        1      2355       5  978824291   \n",
       "\n",
       "                                    title                        genres  \\\n",
       "0  One Flew Over the Cuckoo's Nest (1975)                         Drama   \n",
       "1        James and the Giant Peach (1996)  Animation|Children's|Musical   \n",
       "2                     My Fair Lady (1964)               Musical|Romance   \n",
       "3                  Erin Brockovich (2000)                         Drama   \n",
       "4                    Bug's Life, A (1998)   Animation|Children's|Comedy   \n",
       "\n",
       "  gender  age  occupation    zip  \n",
       "0      F    1          10  48067  \n",
       "1      F    1          10  48067  \n",
       "2      F    1          10  48067  \n",
       "3      F    1          10  48067  \n",
       "4      F    1          10  48067  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e1aedc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
